## 1. 网页抓取

### 1.1初始化
开始网页抓取前，初始化几个重要的变量。
```
used_url_set = set()  # 已经爬取过的网页 url 的集合
to_use_url_list = []  # 接下来将要爬取的网页 url 的列表  
url_id_dic = dict()  # url 到 id 的映射
id_url_dic = dict()  # id 到 url 的映射
id_title_dic = dict()  # id 到 title 的映射
id_jumpUrls_dic = dict()  # id 到可以跳转到的其他url的映射
id_jumpAnchorText_dic = dict()  # id 到可以跳转到其他url的锚文本的映射
```
#### 1.2 网页抓取过程

web网页的抓取可以看作一个不断循环的过程，直到爬取结束或者达到了某种设定条件而停止。
- 每次爬取，从 `to_use_url_list` 中选取一个url，调用`get_url_data`函数从该url对应的网页中获取数据；
- 使用`BeautifulSoup`解析网页，获取页面中的文本、锚文本和链接；
- 爬取结束后，从 `to_use_url_list` 中删除已经处理过的`url`，把它加入 `used_url_set` 中
- 将爬取到的标题和文本转换为`XML`格式，然后进入下一次爬取过程；
```
def content_to_xml(title, content):  
    root = ET.Element("webpage")  
    title_element = ET.SubElement(root, "title")  
    title_element.text = title  
    content_element = ET.SubElement(root, "content")  
    content_element.text = content  
    tree = ET.ElementTree(root)  
    return ET.tostring(root, encoding='unicode')  # 返回 XML 字符串
```
- 如果`to_use_url_list`不为空，就选取一个`url`继续爬取；
#### 1.3保存数据
- 为了减少`io`操作，采用批量写入的策略将标题和文本以`XML`格式的保存到`data/file_set/`；
- 将索引数据以 `.pkl`格式保存到`data/index_set/`。

`pickle` 库用于对象的序列化和反序列化操作，我们可以使用`pickle` 的`dump()`函数保存 `.pkl`文件，使用`load()`函数加载`.pkl`文件。
## 2. 文本索引
标题、锚文本、url部分的索引我们在网页抓取的时候就已经建立好了，所以只需要为文本构建索引。

为用户提供查询服务时，我们希望先对查询字符串进行预处理，将查询字符串转换为查询向量，使用倒排索引快速定位包含特定词项的文档，最后使用TF - IDF 矩阵对词项向量和文档向量的相关性进行量化评估，并返回排序结果。

其中，为文本构建倒排索引可以减少最终计算查询向量和文档向量相似度的维度，我们只需要关注分词出现的相关文档，而不需要为所有文档都计算相似度。

在倒排索引的基础上增加逆文档频率（IDF）的信息，构建TF - IDF 矩阵。

1. 预处理：去掉标点，小写转换，`jieba`分词，去除停用词。
2. 构建倒排索引：倒排索引字典，单词为键，单词的相关信息信息为值，包括单词出现过的文档id、出现次数、出现位置列表，每个元素为一个元组 (起始位置，终止位置)、词频 (TF)、逆文档频率 (IDF)。
```
# 单词的相关信息类
class Invert_term:  
    # 单词出现过的文档id  
    id = None  
    # 单词出现次数  
    count = None  
    # 单词出现位置列表，每个元素为一个元组 (起始位置, 终止位置)  
    positions = []  
    # 词频 (TF)    
    tf = None  
    # 逆文档频率 (IDF)    
    idf = None  
  
  
    def __init__(self, id, max_term_count=1):  
       self.id = id  
       self.count = 0  
       self.positions = []  
       self.max_term_count = max_term_count  
       self.tf = 0.0  
       self.idf = 0.0  
  
  
    def update_tf(self, term_count):  
       # 使用归一化后的词频  
       if self.max_term_count > 0:  
          self.tf = self.count / self.max_term_count  
       else:  
          self.tf = 0.0  
  
    def __repr__(self):  
       return f'[id:{self.id}, count:{self.count}, positions:{self.positions}, tf:{self.tf}, idf:{self.idf}]'
```

- 词频（TF）：词项`t`在文档中出现的次数。计算公式如下：
$$\mathrm{tf_{t,d}~=~\frac{f_{t,d}}{n_d}}$$
其中，$f_{t,d}$表示词项`t`在文档`D`中出现的次数，$n_d$表示文档`D`中的总词数。

为了防止某些词项在长文档中频繁出现而获得过高的权重，我们在`Invert_term`类中增加了`max_term_count`属性，用于记录当前文档中所有词项的最大出现次数。对词频进行归一化处理，将词项的出现次数除以文档中所有词项的最大出现次数。计算公式如下：
$$\mathrm{tf_{t,d}~=~\frac{f_{t,d}}{{max}(n_d)}}$$

倒排索引：
```
{
	'剪影': [
		[
			id:15, 
			count:1, 
			positions:[(290, 292)], 
			tf:0.002688172043010753, 
			idf:3.912023005428146
		]
	], 
	'院徽': [
		[
			id:15, 
			count:1, 
			positions:[(298, 300)], 
			tf:0.002688172043010753, 
			idf:3.912023005428146
		]
	], 
	'正式': [
		[
			id:15, 
			count:4, 
			positions:[(300, 302), (325, 327), (624, 626), (2766, 2768)], 
			tf:0.010752688172043012, 
			idf:2.995732273553991
		], 
		[
			id:16, 
			count:1, 
			positions:[(457, 459)], 
			tf:0.0027247956403269754, 
			idf:2.995732273553991
		], 
		[
			id:25, 
			count:1, 
			positions:[(501, 503)], 
			tf:0.002583979328165375, 
			idf:2.995732273553991
		], 
		[
			id:46, 
			count:3, 
			positions:[(268, 270), (1009, 1011), (1899, 1901)], 
			tf:0.007894736842105263, 
			idf:2.995732273553991
		]
	]
}
```

3. 计算逆文档频率（IDF）。
```
def calculate_idf(inverted_index_dic, file_num):
	# 遍历每个词项，计算其 IDF
	for term, terms_list in inverted_index_dic.items():
		df = len(terms_list)  # 包含该词项的文档数量
		# 使用平滑处理的 IDF 计算公式
        idf = math.log((file_num / (df + 1)))
        for term_item in terms_list:
            term_item.idf = idf
```
IDF的原理是对于某一个特征词条项，包含此词条项的文档数量越少，此词条项就具有越强的文档类别特征。计算公式如下：
$$\mathrm{idf_t}=\log\frac{\mathrm{N}}{\mathrm{df_t}}$$

其中，`N` 是文档总数，${df_t}$ 是文档频率。

- 文档频率（DF）：词项 `t` 出现在多少篇文档中。

为了避免分母为零的情况（即某个词项从未出现在任何文档中），对 `IDF` 进行平滑处理。计算公式如下：
$$\mathrm{idf_t}=\log(\frac{\mathrm{N}}{\mathrm{df_t}+1})$$

4. 构建词汇表字典（词汇到索引的映射）
```
# 词汇表  
vocab = list(inverted_index_dic.keys())  
# 构建词汇表字典（词汇到索引的映射）  
vocab_dic = {term: i for i, term in enumerate(vocab)}
```

词汇表字典
```
vocab = {'剪影': 1796, '院徽': 1797, '正式': 1798}
```

5. 构建 TF-IDF 矩阵（二维矩阵，横列为词项，竖列为文档ID）
```
def build_tfidf_matrix(inverted_index_dic, file_num, vocab_dic):  
    vocab_num = len(vocab_dic)  
      
    tfidf_matrix = np.zeros((file_num, vocab_num))  
  
    for term, terms_list in inverted_index_dic.items():  
       term_index = vocab_dic[term]  
       for term_item in terms_list:  
          doc_id = term_item.id  
          tfidf_matrix[doc_id, term_index] = term_item.tf * term_item.idf  
  
    # 对文档向量（即 TF-IDF 矩阵的每一行）进行 L2 归一化处理  
    tfidf_matrix = normalize(tfidf_matrix, norm='l2')  
  
    return tfidf_matrix, vocab_dic
```
将每个文档表示为一个向量，向量的每个维度对应于词汇表中的一个词项，值为该词项的 TF-IDF 权重。计算公式如下：
$$\mathrm{tf-{idf_{t,d}}~=~tf_{t,d} * idf_t}$$	
其中，${tf_{t,d}}$是词项 `t` 在文档 `d` 中的词频，${idf_t}$是词项 `t` 的逆文档频率。

使用 `sklearn.preprocessing.normalize` 对文档向量（即 TF-IDF 矩阵的每一行）进行 L2 归一化处理，将向量的长度调整为 1。计算公式如下：
$$\mathrm{tf-idf}_{\text{normalized}(t,d)}=\frac{\mathrm{tf-idf_{t,d}}}{\sqrt{\sum_{t^{\prime}\in d}\mathrm({tf-idf_t^{\prime},d})^2}}$$

TF-IDF 矩阵：

| 文档\词项 | term 0 | term 1 | term 2 | term 3 |
| ----- | ------ | ------ | ------ | ------ |
| 文档 0  | 0.1    | 0.0    | 0.2    | 0.0    |
| 文档 1  | 0.05   | 0.15   | 0.0    | 0.1    |
| 文档 2  | 0.0    | 0.2    | 0.0    | 0.3    |

## 3. 链接分析
使⽤PageRank进⾏链接分析，评估⽹⻚权重。

- 我们需要从本地的 `.pkl` 文件中加载`url_id_dic`（url 到 id 的映射）和`id_jumpUrls_dic`（id 到可以跳转到的其他url的映射）
- 创建一个 `networkx` 库中的有向图对象 `graph`
- 根据 url 之间的跳转关系构建一个有向图。通过两层嵌套循环遍历 `id_jumpUrls_dic`，外层循环遍历网页ID，内层循环遍历每个id 到可以跳转到的其他url。对于每个url，如果能找到对应的网页ID，就通过add_edge函数构造一条有向边。
- 使用 `networkx` 库的 `PageRank` 函数对构建好的有向图 `graph` 进行计算。
>`PageRank` 算法基于图的结构（节点和边的关系，在这里就是url之间的跳转关系）来衡量每个节点（网页）的重要性，基本原理是指向一个网页的超链接越多，随机跳转到该网页的概率也就越高，那么该网页的 `PageRank` 值就越高。
- 最后将计算得到的 `PageRank` 字典以 `.pkl`格式保存到`data/index_set/`
## 4. 查询服务
前端在 `/web` 目录下，通过 get 、 post 和后端通信。
#### 4.1注册&登录
后端使用 Flask 框架。
1. 使用 SQLite 数据库存储用户信息和用户查询日志。
2. 用户注册： `/signup` 路由处理用户提交的注册表单数据。检查用户名是否已存在，以避免重复注册。然后在 `users` 表中创建新用户记录，包括 `username`、`email`、`password`、`age` 和 `sex` 字段。
3. 用户登录： `/signin` 路由处理用户提交的登录表单数据。在 `users` 表中查询用户名和密码。验证成功后将用户信息存储在 session 中，以便跟踪用户登录状态。

启动服务，监听5000端口
![[Pasted image 20241209223419.png]]

打开http://127.0.0.1:5000
![[Pasted image 20241209223506.png]]

填写用户信息
![[Pasted image 20241209223621.png]]

点击注册
![[Pasted image 20241209223637.png]]

查看本地数据库`user`表
![[Pasted image 20241209223734.png]]
#### 4.2普通查询
登陆后，进入查询界面
![[Pasted image 20241210024641.png]]
当用户进行查询时，后端需要先加载数据，对查询字符串进行预处理，将查询字符串转换为查询向量，使用倒排索引快速定位包含特定词项的文档，最后使用TF - IDF 矩阵对词项向量和文档向量的相关性进行量化评估，并返回排序结果。具体步骤如下：

1. 加载数据
加载倒排索引、URL 映射、标题映射、TF-IDF 矩阵、词汇映射和PageRank映射
``` 
def load_data():  
    with open(inverted_index_path, "rb") as f:  
        inverted_index_dic = pickle.load(f)  
    with open(id_url_path, "rb") as f:  
        id_url_dic = pickle.load(f)  
    with open(id_title_path, "rb") as f:  
        id_title_dic = pickle.load(f)  
    tfidf_matrix = csr_matrix(np.load(tfidf_matrix_path))  
    with open(vocab_path, "rb") as f:  
        vocab_dic = pickle.load(f)  
    with open(id_pagerank_path, "rb") as f:  
        id_pagerank_dic = pickle.load(f)  
    return inverted_index_dic, id_url_dic, id_title_dic, tfidf_matrix, vocab_dic, id_pagerank_dic
```
2. 预处理
去除停用词、小写转换、使用`jieba`（中文分词）库进行分词。
```
def pretreatment(query_str):
    # 去掉标点，小写，分词
    query_str = query_str.replace(' ', '')
    content = re.sub(r"[{}、，。！？·【】）》；;《“”（-]+".format(punctuation), "", query_str)
    content = content.lower()
    query_terms = jieba.lcut_for_search(content)
    return query_str, query_terms
```

3. 构建查询向量
在本地我们将文档表示为向量（通过 `TF - IDF` 矩阵表示），因此我们应该将查询字符串转化为查询向量，和文档向量在同一个特征空间中进行比较
```
def query_to_vector(query_terms, vocab_dict, inverted_index_dic):
    # 初始化查询向量 query_vector 和词频字典 query_term_freq 
    query_vector = np.zeros(len(vocab_dict))
    query_term_freq = {}

    # 统计查询词项列表中每个词项的频次
    for term in query_terms:
        if term in vocab_dict:
            query_term_freq[term] = query_term_freq.get(term, 0) + 1

    # 使用 TF * IDF 构建查询向量
    for term, freq in query_term_freq.items():
        term_index = vocab_dict[term]
        if term in inverted_index_dic:
            # 使用查询词项列表中的词频作为 TF
            tf = freq / len(query_terms)            
            # 获取词项的 idf 值
            # 理论上不同文档中的同一个词项，它们的 idf 值都是相同的
            # 所以选择了第一个包含该词项的文档项的 idf 值
            idf = inverted_index_dic[term][0].idf
            query_vector[term_index] = tf * idf
        else:
            print(f"查询词项 '{term}' 不在词汇表中")
            query_vector[term_index] = 0

    # 对查询向量进行 L2 归一化
    query_vector = normalize(query_vector.reshape(1, -1), norm='l2').flatten()

    return query_vector
```
- 初始化查询向量`query_vector`和词频字典`query_term_freq`
- 统计查询词项列表中每个词项的频次。
- 构建查询向量。对于每个词项，可以用查询词项列表中的词频作为 TF，
- 遍历词频字典 `query_term_freq`，对于每个词项及其对应的频次，通过词汇表字典 `vocab_dict` 获取该词项在向量中的索引位置 `term_index`。使用查询词项列表中的词频作为 TF，第一个包含该词项的文档项的 idf 值作为 IDF。
- 对查询向量进行 L2 归一化，原理同对文档向量的处理。

```
query_str: 南开大学主校区在南开区
query_term_freq {'南开': 2, '开大': 1, '大学': 1, '南开大学': 1, '主': 1, '校区': 1, '南开区': 1}
query_vector [0.         0.01375009 0.         ... 0.         0.         0.        ]
query_vector归一化 [0.         0.02245022 0.         ... 0.         0.         0.        ]
```

4. 使用倒排索引筛选相关文档
用于快速检索包含查询词项的文档集合
```
def get_relevant_documents(query_terms, inverted_index_dic):
    relevant_doc_ids = set()
    
    for item in query_terms:
        if item in inverted_index_dic.keys():
            for entry in inverted_index_dic[item]:
                relevant_doc_ids.add(entry.id)
                
    return list(relevant_doc_ids)
```
- 创建相关文档id集合 `relevant_doc_ids`，往集合中添加文档ID时会自动去重
- 遍历包含当前词项的文档条目，将文档ID添加到集合中
- 将相关文档id集合转换为列表并返回

5. 计算相似度并进行相关性排序
```
def get_similar_documents(query_vector, tfidf_matrix, id_pagerank_dict, relevant_doc_ids,
                          relevance_threshold=0.02, alpha=0.8):
    # 筛选相关文档的 TF - IDF 矩阵
    relevant_tfidf_matrix = tfidf_matrix[relevant_doc_ids]
    
    # 计算查询向量与相关文档之间的余弦相似度
    cosine_similarity_scores = cosine_similarity([query_vector], relevant_tfidf_matrix).flatten()
    
    # 获取相关文档的 PageRank 分数
    pagerank_scores = np.array([id_pagerank_dict.get(doc_id, 0) for doc_id in relevant_doc_ids])
    
    # 计算加权组合得分
    final_scores = alpha * cosine_similarity_scores + (1 - alpha) * pagerank_scores
    
    # 按得分从高到低排序
    sorted_indexes = np.argsort(final_scores)[::-1]

    # 返回对应的文档 ID
    similar_docs = []
    for i in sorted_indexes:
        if final_scores[i] > relevance_threshold:
            doc_id = relevant_doc_ids[i]
            similar_docs.append(doc_id)
        else:
            break

    return similar_docs

```
- 筛选相关文档的 TF - IDF 矩阵。
- 使用 `scikit-learn` 的 `cosine_similarity` 函数计算查询向量与相关文档向量之间的余弦相似度。
- 获取相关文档的 PageRank 分数
- 计算余弦相似度和 PageRank 的加权组合得分
- 按相似度排序
- 根据阈值进行筛选。一旦遇到相似度分数小于阈值的情况，就停止循环。
- 返回满足相似度阈值要求的相关文档ID列表。


我们提供了4种高级搜索功能
![[Pasted image 20241210024623.png]]
#### 4.3站内查询

 `/instation_query` 路由处理站内查询

用户搜索内容的时候，也指定了域名
![[Pasted image 20241210024548.png]]
1. 解析前端发送的JSON数据，**包括 query_url**
2. 使用 `load_data` 函数加载相关数据
3. 处理查询字符串
	- 预处理（去标点、小写、分词）。
	- 使用 `query_to_vector` 函数将查询字符串转换为查询向量。
4. 使用倒排索引筛选相关文档：使用 `get_relevant_documents` 函数根据查询词项从倒排索引中筛选出相关的文档 ID。
5. 计算相似度并获取最相关的文档：使用 `get_similar_documents` 函数计算查询向量与筛选后的每个文档向量之间的余弦相似度，并返回相似度超过阈值的文档。
6. **检查 URL 是否匹配，限制搜索结果只返回与该 URL 相关的文档**
```
# 解析 query_url 和 id_url_dic[doc_id]
parsed_query_url = urlparse(query_url) if query_url else None
parsed_doc_url = urlparse(id_url_dic[doc_id])

if (parsed_query_url and (parsed_query_url.netloc == parsed_doc_url.netloc) and  
                     (parsed_doc_url.path.startswith(parsed_query_url.path))):
```
使用`urllib.parse`的`urlparse`函数，将 URL 字符串解析为一个 `ParseResult` 对象。该对象包含协议、域名（以及可能的端口号）、路径、参数、参数、片段标识符等

因此，`http://news.nankai.edu.cn/gynk/system`会被解析为：
```
scheme: http 
netloc: news.nankai.edu.cn 
path: /gynk/system 
params: 
query: 
fragment:
```

先检查`parsed_query_url`是否存在，然后判断域名（`netloc`）是否相等，最后判断路径是否以`path`开头。

7. 返回结果：包含文档标题、内容摘要、URL 等信息的结果列表。
#### 4.4文档查询

 `/document_query` 路由处理文档查询


#### 4.5短语查询

 `/phrase_query` 路由处理短语查询
 
普通查询，一共有13条记录
![[Pasted image 20241210024430.png]]

短语查询，一共有4条记录
![[Pasted image 20241210024406.png]]
1. 解析前端发送的JSON数据
2. 使用 `load_data` 函数加载相关数据
3. 处理查询字符串
	- 预处理（去标点、小写、分词）。
	- 使用 `query_to_vector` 函数将查询字符串转换为向量。
4. **使用`get_relevant_documents_dp`函数动态规划筛选短语连续的文档**
```
# 使用动态规划筛选短语连续的文档
def get_relevant_documents_dp(query_terms, inverted_index_dic, id_title_dic):
    # 维护一个二维数组 dp[i][j]，其中 i 表示查询的词项的索引，j 表示文档的 ID。
    # dp[i][j] 的值为 True，表示查询的第 i 个词项可以与前一个词项在文档 j 中连续匹配
    file_num = len(id_title_dic)
    dp = [[False] * file_num for _ in range(len(query_terms))]
    
    # 处理第一个分词
    for term_entry in inverted_index_dic.get(query_terms[0], []):
        doc_id = term_entry.id
        positions = term_entry.positions
        
        # 将所有包含该词项的文档 dp[0][id] 设置为 True
        for start_pos, end_pos in positions:
            dp[0][doc_id] = True
    
    # 遍历后续分词
    for i in range(1, len(query_terms)):
        for term_entry in inverted_index_dic.get(query_terms[i], []):
            doc_id = term_entry.id
            positions = term_entry.positions
            
            # 检查当前分词结果的起始位置是否不晚于前一个分词结果的结束位置
            for start_pos, end_pos in positions:
                if dp[i - 1][doc_id]:
                    prev_positions = [pos for pos in inverted_index_dic[query_terms[i - 1]] if pos.id == doc_id]
                    for prev_start_pos, prev_end_pos in prev_positions[0].positions:
                        if start_pos <= prev_end_pos:
                            dp[i][doc_id] = True
                            break
    # 匹配成功的文档
    matched_doc_ids = [i for i in range(file_num) if dp[len(query_terms) - 1][i]]
    
    return matched_doc_ids
```
	- 初始化：使用一个二维数组 `dp[i][j]` 来记录查询的第 `i` 个词项是否可以在文档 `j` 中与前一个词项连续匹配。
	- 处理第一个分词：对于第一个分词，我们只需检查它是否出现在文档中，并将相应的 `dp[0][doc_id]` 设置为 `True`。
	- 处理后续分词：对于后续的分词，我们不仅要检查它是否出现在文档中，还要确保它的起始位置不晚于前一个分词结果的结束位置。
5. 计算相似度并获取最相关的文档：使用 `get_similar_documents`  函数计算查询向量与动态规划筛选后的每个文档向量之间的余弦相似度，并返回相似度超过阈值的文档。
6. 返回结果：包含文档标题、内容摘要、URL 等信息的结果列表。

#### 4.6通配查询

 `/wildcard_query` 路由处理通配查询
 
普通查询，不能处理
![[Pasted image 20241210003143.png]]

通配查询，可以处理
![[Pasted image 20241210024304.png]]
1. 解析前端发送的JSON数据
2. 使用 `load_data` 函数加载相关数据
3. 处理查询字符串
	- 预处理（去标点、小写、分词）。
	- **使用`fnmatch`库的 `fnmatch` 函数进行通配符查询，支持的通配符为“\*”（匹配任意字符序列，包括空字符序列）以及"？"（匹配任意单字符）。**
```
for item in list(inverted_index_dic.keys()):
    if fnmatch(item, query_str):
        query_terms.append(item)
```
	- 使用 `query_to_vector` 函数将查询字符串转换为向量。
4. 使用倒排索引筛选相关文档：使用 `get_relevant_documents` 函数根据查询词项从倒排索引中筛选出相关的文档 ID。
5. 计算相似度并获取最相关的文档：使用  `get_similar_documents`  函数计算查询向量与筛选后的每个文档向量之间的余弦相似度，并返回相似度超过阈值的文档。
6. 返回结果：包含文档标题、内容摘要、URL 等信息的结果列表。

#### 4.7查询日志

点击搜索框，会出现用户的搜索历史，可以进行搜索，也可以删除搜索历史项。
![[Pasted image 20241210004938.png]]

查看本地数据库`query_logs`表，`user_id`为4的用户确实有4条查询日志
![[Pasted image 20241210005428.png]]

删除两条日志
![[Pasted image 20241210005655.png]]

`query_logs`表`user_id`为4的用户的查询日志也减少了两条
![[Pasted image 20241210005735.png]]

用户进行查询的时候，后端处理查询的路由都会调用`update_query_logs`函数更新数据库的 `query_logs` 表

用户删除查询日志项，由后端的`/signin` 路由进行处理。

#### 4.8⽹⻚快照
网页快照是搜索引擎在收录网页时，对该网页进行索引，然后存入服务器缓存里，当用户在搜索引擎中点击链接时，搜索引擎将爬虫系统当时所抓取并保存的网页内容展现出来。

点击“查看快照”按钮
![[Pasted image 20241210024227.png]]

会在新标签页中打开快照
![[Pasted image 20241210022822.png]]

在`spider.py`文件中
```
# 配置ChromeDriver的路径
service = Service('C:/Users/XU/AppData/Local/Google/Chrome/Application/chromedriver.exe')

# 设置Chrome选项
chrome_options = Options()
chrome_options.add_argument('--headless') # 无头模式
chrome_options.add_argument('--no-sandbox') # 解决DevToolsActivePort文件不存在的报错
chrome_options.add_argument('--disable-dev-shm-usage')  # 解决资源限制问题

# 创建WebDriver实例
driver = webdriver.Chrome(service=service, options=chrome_options)
```
配置 WebDriver 和 ChromeDriver：`WebDriver` 是 Selenium 提供的一个接口，用于与浏览器进行交互。`ChromeDriver` 是 Chrome 浏览器的驱动程序，允许 Selenium 控制 Chrome 浏览器的行为。

```
	try:
		driver.get(url)
		# 使用显式等待，等待页面的<body>元素加载完成
		wait = WebDriverWait(driver, 10)  # 设置最长等待时间为10秒
		wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
		
		# 获取整个页面的高度和宽度
		total_width = driver.execute_script("return document.documentElement.scrollWidth")
		total_height = driver.execute_script("return document.documentElement.scrollHeight")
		
		# 设置浏览器窗口大小以适应整个页面
		driver.set_window_size(total_width, total_height)
		
		# 捕获整个页面的截图
		screenshot_as_png = driver.get_screenshot_as_png()
		image_stream = io.BytesIO(screenshot_as_png)
		stitched_image = Image.open(image_stream)
		
		# 保存最终截图
		screenshot_filename = str(count) + '.png'
		screenshot_path = os.path.join('web/static/snap_shot', screenshot_filename)
		os.makedirs(os.path.dirname(screenshot_path), exist_ok=True)
		stitched_image.save(screenshot_path)
	except:
		print('网页截图失败')

```
根据传入的 `URL`，使用 `Selenium` 库配合 `ChromeDriver` 来加载对应的网页，显式等待页面加载完成后，调整浏览器窗口大小以适配整个页面内容，然后捕获整个页面的截图，并最终将截图保存到本地服务器指定位置。

用户进行查询的时候，后端处理查询的路由都会将`snap_shot_path`作为可访问的 `URL`返回给前端。

用户在前端点击快照链接后，会在新标签页中打开快照。
#### 4.9个性化查询
在`query_to_vector`函数中，将用户信息（用户年龄、学院）、查询日志转化为向量，结合查询向量按照给定权重构建最终查询向量。

将用户的年龄分组为本科生、研究生、博士生和老师四类，并将其作为独立的词项融入查询向量。
```
# 用户年龄转化为向量（词频设为1）
age_vector = np.zeros(len(vocab_dic))
if user_age:
    age_group = ""
    if 18 <= user_age <= 22:
        age_group = "本科生"
    elif 22 < user_age <= 25:
        age_group = "研究生"
    elif 25 < user_age <= 30:
        age_group = "博士生"
    elif user_age >= 30:
        age_group = "老师"
    
    age_term = f"age_{age_group}"
    if age_term in vocab_dic:
                age_vector[vocab_dic[age_term]] = 1
    
# 对年龄向量进行 L2 归一化处理
age_vector = normalize(age_vector.reshape(1, -1), norm='l2').flatten()
```

学院名称直接作为词项融入查询向量。
```
# 用户学院转化为向量（词频设为1）
academy_vector = np.zeros(len(vocab_dic))
if user_academy:
    academy_term = f"academy_{user_academy}"
    if academy_term in vocab_dic:
                academy_vector[vocab_dic[academy_term]] = 1
    
# 对学院向量进行 L2 归一化处理
academy_vector = normalize(academy_vector.reshape(1, -1), norm='l2').flatten()
```


查询日志只取最近10条查询日志，采用时间衰减原则，根据每条日志的实际时间戳来计算衰减权重。
```
# 查询日志转化为向量（取最近10条，采用时间衰减原则）
log_vector = np.zeros(len(vocab_dic))
if query_logs:
    current_time = datetime.now()  # 获取当前时间
    decay_rate = 0.1  # 时间衰减率
    for log in query_logs[:10]:  # 只取最近10条查询日志
        log_query_str = log['query_str']
        log_terms = pretreatment(log_query_str)
        time_diff = (current_time - log['timestamp']).total_seconds() / (60 * 60 * 24)  # 时间差以天为单位
        weight = math.exp(-decay_rate * time_diff)  # 指数衰减
        
        for term in log_terms:
            if term in vocab_dic:
                log_vector[vocab_dic[term]] += weight
    
# 对查询日志向量进行 L2 归一化处理
log_vector = normalize(log_vector.reshape(1, -1), norm='l2').flatten()
```

按照给定权重融合各部分向量，构建最终查询向量
```
final_query_vector = (
    alpha * query_vector + 
    beta * age_vector + 
    gamma * academy_vector + 
    delta * log_vector
)
# 对最终查询向量进行 L2 归一化处理
final_query_vector = normalize(final_query_vector.reshape(1, -1), norm='l2').flatten()
```
#### 4.10个性化推荐

 `/get_html/<int:doc_id>` 路由处理站内查询
 
用户点击进入某一网页，系统会在底部进行相关个性化推荐，用户点击链接可以继续跳转。
![[Pasted image 20241211022501.png]]

1. 使用 `load_data` 函数加载相关数据。
2. 将当前文档的标题、用户信息（用户年龄、学院）、查询日志转化为向量。
3. 使用倒排索引筛选相关文档：使用 `get_relevant_documents` 函数根据查询词项从倒排索引中筛选出相关的文档 ID。
4. 计算相似度并获取最相关的文档：使用 `get_similar_documents` 函数计算查询向量与筛选后的每个文档向量之间的余弦相似度，并返回相似度超过阈值的文档。
5. 筛选出推荐文档列表。
6. 使用 `xml.etree.ElementTree` 的 **`fromstring()`** 和 **`findtext()`** 函数解析 XML 文件中的标题和内容。
7. 使用`Jinja2` 模板引擎的`render_template_string`函数动态生成 HTML 内容。



